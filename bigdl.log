2019-10-24 00:32:16 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:33:42 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:36:52 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:40:12 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:41:17 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:43:44 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:44:21 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:44:56 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:45:22 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:48:10 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:48:35 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:48:56 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:49:13 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:49:38 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:51:39 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:52:46 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:53:07 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:53:33 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:53:55 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:54:21 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 00:54:51 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:05:43 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:07:27 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:08:17 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:08:58 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:09:45 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:10:10 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:10:22 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:10:22 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:10:23 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-24 01:10:23 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-24 01:10:24 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-24 01:10:24 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-24 01:10:24 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-24 01:10:24 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-24 01:10:24 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-24 01:10:24 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-24 01:10:24 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-24 01:10:24 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-24 01:10:24 INFO  DistriOptimizer$:148 - Count dataset
2019-10-24 01:10:24 INFO  DistriOptimizer$:148 - Count dataset
2019-10-24 01:10:24 ERROR DistriOptimizer$:907 - Error: java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1037)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1037)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1037)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:150)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:427)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:10:24 ERROR DistriOptimizer$:907 - Error: java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1037)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1037)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1037)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:150)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:427)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:12:02 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:14:28 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:14:40 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:14:40 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:14:42 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-24 01:14:42 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-24 01:14:43 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-24 01:14:43 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-24 01:14:43 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-24 01:14:43 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-24 01:14:43 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-24 01:14:43 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-24 01:14:43 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-24 01:14:43 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-24 01:14:43 INFO  DistriOptimizer$:148 - Count dataset
2019-10-24 01:14:43 INFO  DistriOptimizer$:148 - Count dataset
2019-10-24 01:14:43 ERROR DistriOptimizer$:907 - Error: java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1037)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1037)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1037)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:150)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:449)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:14:43 ERROR DistriOptimizer$:907 - Error: java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1037)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1037)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1037)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:150)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:449)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:24:20 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:24:43 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:24:55 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:24:55 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:24:56 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-24 01:24:56 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-24 01:24:57 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-24 01:24:57 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-24 01:24:57 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-24 01:24:57 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-24 01:24:57 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-24 01:24:57 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-24 01:24:57 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-24 01:24:57 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-24 01:24:57 INFO  DistriOptimizer$:148 - Count dataset
2019-10-24 01:24:57 INFO  DistriOptimizer$:148 - Count dataset
2019-10-24 01:24:57 ERROR DistriOptimizer$:907 - Error: java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1037)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1037)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1037)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:150)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:449)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:24:57 ERROR DistriOptimizer$:907 - Error: java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1037)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1037)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1037)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:150)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:449)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:26:06 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:26:17 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:26:17 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:26:18 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-24 01:26:18 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-24 01:26:19 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-24 01:26:19 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-24 01:26:19 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-24 01:26:19 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-24 01:26:19 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-24 01:26:19 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-24 01:26:19 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-24 01:26:19 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-24 01:26:19 INFO  DistriOptimizer$:148 - Count dataset
2019-10-24 01:26:19 INFO  DistriOptimizer$:148 - Count dataset
2019-10-24 01:26:19 ERROR DistriOptimizer$:907 - Error: java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1037)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1037)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1037)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:150)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:449)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:26:19 ERROR DistriOptimizer$:907 - Error: java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1037)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1037)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1037)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:150)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:449)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:27:48 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:28:02 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:29:09 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:29:22 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:29:22 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:29:25 ERROR Executor:91 - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArrayIndexOutOfBoundsException: 3
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$$anonfun$transform$1.apply$mcVI$sp(ChannelNormalize.scala:76)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$.transform(ChannelNormalize.scala:74)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize.transformMat(ChannelNormalize.scala:34)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer.transform(FeatureTransformer.scala:65)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-24 01:29:25 ERROR Executor:91 - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArrayIndexOutOfBoundsException: 3
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$$anonfun$transform$1.apply$mcVI$sp(ChannelNormalize.scala:76)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$.transform(ChannelNormalize.scala:74)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize.transformMat(ChannelNormalize.scala:34)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer.transform(FeatureTransformer.scala:65)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-24 01:29:25 WARN  TaskSetManager:66 - Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 3
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$$anonfun$transform$1.apply$mcVI$sp(ChannelNormalize.scala:76)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$.transform(ChannelNormalize.scala:74)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize.transformMat(ChannelNormalize.scala:34)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer.transform(FeatureTransformer.scala:65)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:29:25 WARN  TaskSetManager:66 - Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 3
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$$anonfun$transform$1.apply$mcVI$sp(ChannelNormalize.scala:76)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$.transform(ChannelNormalize.scala:74)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize.transformMat(ChannelNormalize.scala:34)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer.transform(FeatureTransformer.scala:65)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:29:25 ERROR TaskSetManager:70 - Task 0 in stage 0.0 failed 1 times; aborting job
2019-10-24 01:29:25 ERROR TaskSetManager:70 - Task 0 in stage 0.0 failed 1 times; aborting job
2019-10-24 01:31:09 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:31:22 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:31:22 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:31:24 ERROR Executor:91 - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArrayIndexOutOfBoundsException: 3
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$$anonfun$transform$1.apply$mcVI$sp(ChannelNormalize.scala:76)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$.transform(ChannelNormalize.scala:74)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize.transformMat(ChannelNormalize.scala:34)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer.transform(FeatureTransformer.scala:65)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-24 01:31:24 ERROR Executor:91 - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArrayIndexOutOfBoundsException: 3
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$$anonfun$transform$1.apply$mcVI$sp(ChannelNormalize.scala:76)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$.transform(ChannelNormalize.scala:74)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize.transformMat(ChannelNormalize.scala:34)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer.transform(FeatureTransformer.scala:65)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-24 01:31:24 WARN  TaskSetManager:66 - Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 3
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$$anonfun$transform$1.apply$mcVI$sp(ChannelNormalize.scala:76)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$.transform(ChannelNormalize.scala:74)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize.transformMat(ChannelNormalize.scala:34)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer.transform(FeatureTransformer.scala:65)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:31:24 WARN  TaskSetManager:66 - Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 3
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$$anonfun$transform$1.apply$mcVI$sp(ChannelNormalize.scala:76)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$.transform(ChannelNormalize.scala:74)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize.transformMat(ChannelNormalize.scala:34)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer.transform(FeatureTransformer.scala:65)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:31:24 ERROR TaskSetManager:70 - Task 0 in stage 0.0 failed 1 times; aborting job
2019-10-24 01:31:24 ERROR TaskSetManager:70 - Task 0 in stage 0.0 failed 1 times; aborting job
2019-10-24 01:33:50 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:34:02 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:34:02 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:34:05 ERROR Executor:91 - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArrayIndexOutOfBoundsException: 3
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$$anonfun$transform$1.apply$mcVI$sp(ChannelNormalize.scala:76)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$.transform(ChannelNormalize.scala:74)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize.transformMat(ChannelNormalize.scala:34)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer.transform(FeatureTransformer.scala:65)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-24 01:34:05 ERROR Executor:91 - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArrayIndexOutOfBoundsException: 3
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$$anonfun$transform$1.apply$mcVI$sp(ChannelNormalize.scala:76)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$.transform(ChannelNormalize.scala:74)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize.transformMat(ChannelNormalize.scala:34)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer.transform(FeatureTransformer.scala:65)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-24 01:34:05 WARN  TaskSetManager:66 - Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 3
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$$anonfun$transform$1.apply$mcVI$sp(ChannelNormalize.scala:76)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$.transform(ChannelNormalize.scala:74)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize.transformMat(ChannelNormalize.scala:34)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer.transform(FeatureTransformer.scala:65)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:34:05 WARN  TaskSetManager:66 - Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 3
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$$anonfun$transform$1.apply$mcVI$sp(ChannelNormalize.scala:76)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize$.transform(ChannelNormalize.scala:74)
	at com.intel.analytics.bigdl.transform.vision.image.augmentation.ChannelNormalize.transformMat(ChannelNormalize.scala:34)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer.transform(FeatureTransformer.scala:65)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at com.intel.analytics.bigdl.transform.vision.image.FeatureTransformer$$anonfun$apply$1.apply(FeatureTransformer.scala:91)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:34:05 ERROR TaskSetManager:70 - Task 0 in stage 0.0 failed 1 times; aborting job
2019-10-24 01:34:05 ERROR TaskSetManager:70 - Task 0 in stage 0.0 failed 1 times; aborting job
2019-10-24 01:35:34 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:35:46 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:35:46 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:35:50 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-24 01:35:50 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-24 01:35:50 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-24 01:35:50 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-24 01:35:50 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-24 01:35:50 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-24 01:35:50 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-24 01:35:50 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-24 01:35:51 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-24 01:35:51 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-24 01:35:51 INFO  DistriOptimizer$:148 - Count dataset
2019-10-24 01:35:51 INFO  DistriOptimizer$:148 - Count dataset
2019-10-24 01:35:51 ERROR Executor:91 - Exception in task 0.0 in stage 5.0 (TID 3)
java.lang.NullPointerException
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:340)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1021)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1019)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-24 01:35:51 ERROR Executor:91 - Exception in task 0.0 in stage 5.0 (TID 3)
java.lang.NullPointerException
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:340)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1021)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1019)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-24 01:35:51 WARN  TaskSetManager:66 - Lost task 0.0 in stage 5.0 (TID 3, localhost, executor driver): java.lang.NullPointerException
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:340)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1021)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1019)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:35:51 WARN  TaskSetManager:66 - Lost task 0.0 in stage 5.0 (TID 3, localhost, executor driver): java.lang.NullPointerException
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:340)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1021)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1019)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:35:51 ERROR TaskSetManager:70 - Task 0 in stage 5.0 failed 1 times; aborting job
2019-10-24 01:35:51 ERROR TaskSetManager:70 - Task 0 in stage 5.0 failed 1 times; aborting job
2019-10-24 01:35:51 ERROR DistriOptimizer$:907 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 3, localhost, executor driver): java.lang.NullPointerException
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:340)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1021)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1019)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:150)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:449)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:340)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1021)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1019)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-10-24 01:35:51 ERROR DistriOptimizer$:907 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 3, localhost, executor driver): java.lang.NullPointerException
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:340)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1021)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1019)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:150)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:449)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:340)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1021)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1019)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-10-24 01:36:40 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:36:53 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:36:53 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:36:57 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-24 01:36:57 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-24 01:36:57 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-24 01:36:57 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-24 01:36:57 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-24 01:36:57 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-24 01:36:57 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-24 01:36:57 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-24 01:36:57 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-24 01:36:57 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-24 01:36:57 INFO  DistriOptimizer$:148 - Count dataset
2019-10-24 01:36:57 INFO  DistriOptimizer$:148 - Count dataset
2019-10-24 01:36:57 ERROR Executor:91 - Exception in task 0.0 in stage 5.0 (TID 3)
java.lang.NullPointerException
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:340)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1021)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1019)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-24 01:36:57 ERROR Executor:91 - Exception in task 0.0 in stage 5.0 (TID 3)
java.lang.NullPointerException
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:340)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1021)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1019)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-24 01:36:57 WARN  TaskSetManager:66 - Lost task 0.0 in stage 5.0 (TID 3, localhost, executor driver): java.lang.NullPointerException
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:340)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1021)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1019)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:36:57 WARN  TaskSetManager:66 - Lost task 0.0 in stage 5.0 (TID 3, localhost, executor driver): java.lang.NullPointerException
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:340)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1021)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1019)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:36:57 ERROR TaskSetManager:70 - Task 0 in stage 5.0 failed 1 times; aborting job
2019-10-24 01:36:57 ERROR TaskSetManager:70 - Task 0 in stage 5.0 failed 1 times; aborting job
2019-10-24 01:36:57 ERROR DistriOptimizer$:907 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 3, localhost, executor driver): java.lang.NullPointerException
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:340)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1021)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1019)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:150)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:449)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:340)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1021)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1019)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-10-24 01:36:57 ERROR DistriOptimizer$:907 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 3, localhost, executor driver): java.lang.NullPointerException
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:340)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1021)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1019)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:150)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:449)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:340)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1334)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1021)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:1019)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-10-24 01:37:33 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:37:43 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:37:54 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:37:54 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:37:59 ERROR Executor:91 - Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 460, in _load_stream_without_unbatching
    " in batches: (%d, %d)" % (len(key_batch), len(val_batch)))
ValueError: Can not deserialize PairRDD with different number of items in batches: (16, 15)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-24 01:37:59 ERROR Executor:91 - Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 460, in _load_stream_without_unbatching
    " in batches: (%d, %d)" % (len(key_batch), len(val_batch)))
ValueError: Can not deserialize PairRDD with different number of items in batches: (16, 15)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-24 01:37:59 WARN  TaskSetManager:66 - Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 460, in _load_stream_without_unbatching
    " in batches: (%d, %d)" % (len(key_batch), len(val_batch)))
ValueError: Can not deserialize PairRDD with different number of items in batches: (16, 15)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:37:59 WARN  TaskSetManager:66 - Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 460, in _load_stream_without_unbatching
    " in batches: (%d, %d)" % (len(key_batch), len(val_batch)))
ValueError: Can not deserialize PairRDD with different number of items in batches: (16, 15)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:37:59 ERROR TaskSetManager:70 - Task 0 in stage 0.0 failed 1 times; aborting job
2019-10-24 01:37:59 ERROR TaskSetManager:70 - Task 0 in stage 0.0 failed 1 times; aborting job
2019-10-24 01:38:30 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:38:42 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:38:42 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:38:47 ERROR Executor:91 - Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 460, in _load_stream_without_unbatching
    " in batches: (%d, %d)" % (len(key_batch), len(val_batch)))
ValueError: Can not deserialize PairRDD with different number of items in batches: (16, 15)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-24 01:38:47 ERROR Executor:91 - Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 460, in _load_stream_without_unbatching
    " in batches: (%d, %d)" % (len(key_batch), len(val_batch)))
ValueError: Can not deserialize PairRDD with different number of items in batches: (16, 15)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-24 01:38:47 WARN  TaskSetManager:66 - Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 460, in _load_stream_without_unbatching
    " in batches: (%d, %d)" % (len(key_batch), len(val_batch)))
ValueError: Can not deserialize PairRDD with different number of items in batches: (16, 15)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:38:47 WARN  TaskSetManager:66 - Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 377, in main
    process()
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 372, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 393, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/home/root1/work/train-code/venv/lib/python2.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py", line 460, in _load_stream_without_unbatching
    " in batches: (%d, %d)" % (len(key_batch), len(val_batch)))
ValueError: Can not deserialize PairRDD with different number of items in batches: (16, 15)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)
	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:38:47 ERROR TaskSetManager:70 - Task 0 in stage 0.0 failed 1 times; aborting job
2019-10-24 01:38:47 ERROR TaskSetManager:70 - Task 0 in stage 0.0 failed 1 times; aborting job
2019-10-24 01:39:25 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:39:36 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:39:36 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:39:43 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-24 01:39:43 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-24 01:39:43 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-24 01:39:43 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-24 01:39:43 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-24 01:39:43 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-24 01:39:43 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-24 01:39:43 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-24 01:39:43 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-24 01:39:43 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-24 01:39:43 INFO  DistriOptimizer$:148 - Count dataset
2019-10-24 01:39:43 INFO  DistriOptimizer$:148 - Count dataset
2019-10-24 01:39:43 INFO  DistriOptimizer$:152 - Count dataset complete. Time elapsed: 0.086379339s
2019-10-24 01:39:43 INFO  DistriOptimizer$:152 - Count dataset complete. Time elapsed: 0.086379339s
2019-10-24 01:39:43 INFO  DistriOptimizer$:160 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-10-24 01:39:43 INFO  DistriOptimizer$:160 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-10-24 01:39:43 INFO  DistriOptimizer$:164 - Shuffle data
2019-10-24 01:39:43 INFO  DistriOptimizer$:164 - Shuffle data
2019-10-24 01:39:43 INFO  DistriOptimizer$:167 - Shuffle data complete. Takes 0.033156802s
2019-10-24 01:39:43 INFO  DistriOptimizer$:167 - Shuffle data complete. Takes 0.033156802s
2019-10-24 01:39:46 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
Layer info: Model[44c23f62]/KerasLayerWrapper[GraphNetec0ef26d_wrapper]/nn.Scale/CMul[28360a22]([1, 192, 1, 1])
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resizeAs(DenseTensor.scala:2464)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resizeAs(DenseTensor.scala:95)
	at com.intel.analytics.bigdl.nn.CMul.updateOutput(CMul.scala:64)
	at com.intel.analytics.bigdl.nn.CMul.updateOutput(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:50)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:50)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-24 01:39:46 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
Layer info: Model[44c23f62]/KerasLayerWrapper[GraphNetec0ef26d_wrapper]/nn.Scale/CMul[28360a22]([1, 192, 1, 1])
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resizeAs(DenseTensor.scala:2464)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resizeAs(DenseTensor.scala:95)
	at com.intel.analytics.bigdl.nn.CMul.updateOutput(CMul.scala:64)
	at com.intel.analytics.bigdl.nn.CMul.updateOutput(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:50)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:50)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-24 01:39:46 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): Layer info: Model[44c23f62]/KerasLayerWrapper[GraphNetec0ef26d_wrapper]/nn.Scale/CMul[28360a22]([1, 192, 1, 1])
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resizeAs(DenseTensor.scala:2464)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resizeAs(DenseTensor.scala:95)
	at com.intel.analytics.bigdl.nn.CMul.updateOutput(CMul.scala:64)
	at com.intel.analytics.bigdl.nn.CMul.updateOutput(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:50)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:50)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:39:46 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): Layer info: Model[44c23f62]/KerasLayerWrapper[GraphNetec0ef26d_wrapper]/nn.Scale/CMul[28360a22]([1, 192, 1, 1])
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resizeAs(DenseTensor.scala:2464)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resizeAs(DenseTensor.scala:95)
	at com.intel.analytics.bigdl.nn.CMul.updateOutput(CMul.scala:64)
	at com.intel.analytics.bigdl.nn.CMul.updateOutput(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:50)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:50)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-24 01:39:46 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-10-24 01:39:46 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-10-24 01:39:47 ERROR DistriOptimizer$:907 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): Layer info: Model[44c23f62]/KerasLayerWrapper[GraphNetec0ef26d_wrapper]/nn.Scale/CMul[28360a22]([1, 192, 1, 1])
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resizeAs(DenseTensor.scala:2464)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resizeAs(DenseTensor.scala:95)
	at com.intel.analytics.bigdl.nn.CMul.updateOutput(CMul.scala:64)
	at com.intel.analytics.bigdl.nn.CMul.updateOutput(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:50)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:50)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:333)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:427)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Model[44c23f62]/KerasLayerWrapper[GraphNetec0ef26d_wrapper]/nn.Scale/CMul[28360a22]([1, 192, 1, 1])
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resizeAs(DenseTensor.scala:2464)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resizeAs(DenseTensor.scala:95)
	at com.intel.analytics.bigdl.nn.CMul.updateOutput(CMul.scala:64)
	at com.intel.analytics.bigdl.nn.CMul.updateOutput(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:50)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:50)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-10-24 01:39:47 ERROR DistriOptimizer$:907 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): Layer info: Model[44c23f62]/KerasLayerWrapper[GraphNetec0ef26d_wrapper]/nn.Scale/CMul[28360a22]([1, 192, 1, 1])
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resizeAs(DenseTensor.scala:2464)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resizeAs(DenseTensor.scala:95)
	at com.intel.analytics.bigdl.nn.CMul.updateOutput(CMul.scala:64)
	at com.intel.analytics.bigdl.nn.CMul.updateOutput(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:50)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:50)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:333)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:427)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Model[44c23f62]/KerasLayerWrapper[GraphNetec0ef26d_wrapper]/nn.Scale/CMul[28360a22]([1, 192, 1, 1])
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resizeAs(DenseTensor.scala:2464)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resizeAs(DenseTensor.scala:95)
	at com.intel.analytics.bigdl.nn.CMul.updateOutput(CMul.scala:64)
	at com.intel.analytics.bigdl.nn.CMul.updateOutput(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:50)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:50)
	at com.intel.analytics.bigdl.nn.Scale.updateOutput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-10-24 01:40:12 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-24 01:40:25 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:40:25 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-24 01:40:31 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-24 01:40:31 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-24 01:40:32 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-24 01:40:32 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-24 01:40:32 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-24 01:40:32 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-24 01:40:32 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-24 01:40:32 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-24 01:40:32 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-24 01:40:32 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-24 01:40:32 INFO  DistriOptimizer$:148 - Count dataset
2019-10-24 01:40:32 INFO  DistriOptimizer$:148 - Count dataset
2019-10-24 01:40:32 INFO  DistriOptimizer$:152 - Count dataset complete. Time elapsed: 0.094964323s
2019-10-24 01:40:32 INFO  DistriOptimizer$:152 - Count dataset complete. Time elapsed: 0.094964323s
2019-10-24 01:40:32 INFO  DistriOptimizer$:160 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-10-24 01:40:32 INFO  DistriOptimizer$:160 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-10-24 01:40:32 INFO  DistriOptimizer$:164 - Shuffle data
2019-10-24 01:40:32 INFO  DistriOptimizer$:164 - Shuffle data
2019-10-24 01:40:32 INFO  DistriOptimizer$:167 - Shuffle data complete. Takes 0.037406615s
2019-10-24 01:40:32 INFO  DistriOptimizer$:167 - Shuffle data complete. Takes 0.037406615s
2019-10-24 01:40:39 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resize(DenseTensor.scala:2473)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resize(DenseTensor.scala:125)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$.sum(DenseTensorMath.scala:467)
	at com.intel.analytics.bigdl.tensor.DenseTensor.sum(DenseTensor.scala:819)
	at com.intel.analytics.bigdl.nn.CMul.accGradParameters(CMul.scala:153)
	at com.intel.analytics.bigdl.nn.CMul.accGradParameters(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:285)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:62)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateGradInput(NetUtils.scala:70)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-24 01:40:39 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resize(DenseTensor.scala:2473)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resize(DenseTensor.scala:125)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$.sum(DenseTensorMath.scala:467)
	at com.intel.analytics.bigdl.tensor.DenseTensor.sum(DenseTensor.scala:819)
	at com.intel.analytics.bigdl.nn.CMul.accGradParameters(CMul.scala:153)
	at com.intel.analytics.bigdl.nn.CMul.accGradParameters(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:285)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:62)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateGradInput(NetUtils.scala:70)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-24 01:40:43 ERROR Utils:91 - Uncaught exception in thread driver-heartbeater
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-24 01:40:43 ERROR Utils:91 - Uncaught exception in thread driver-heartbeater
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 02:52:35 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-28 02:52:43 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-28 02:52:43 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-28 02:52:47 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-28 02:52:47 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-28 02:52:47 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-28 02:52:47 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-28 02:52:47 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-28 02:52:47 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-28 02:52:47 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-28 02:52:47 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-28 02:52:47 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-28 02:52:47 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-28 02:52:47 INFO  DistriOptimizer$:148 - Count dataset
2019-10-28 02:52:47 INFO  DistriOptimizer$:148 - Count dataset
2019-10-28 02:52:47 INFO  DistriOptimizer$:152 - Count dataset complete. Time elapsed: 0.052997045s
2019-10-28 02:52:47 INFO  DistriOptimizer$:152 - Count dataset complete. Time elapsed: 0.052997045s
2019-10-28 02:52:47 INFO  DistriOptimizer$:160 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-10-28 02:52:47 INFO  DistriOptimizer$:160 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-10-28 02:52:47 INFO  DistriOptimizer$:164 - Shuffle data
2019-10-28 02:52:47 INFO  DistriOptimizer$:164 - Shuffle data
2019-10-28 02:52:47 INFO  DistriOptimizer$:167 - Shuffle data complete. Takes 0.025931781s
2019-10-28 02:52:47 INFO  DistriOptimizer$:167 - Shuffle data complete. Takes 0.025931781s
2019-10-28 02:52:53 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.lang.Float.valueOf(Float.java:433)
	at scala.runtime.BoxesRunTime.boxToFloat(BoxesRunTime.java:73)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.sum(TensorNumeric.scala:503)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$$anonfun$sum$2.apply(DenseTensorMath.scala:470)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$$anonfun$sum$2.apply(DenseTensorMath.scala:469)
	at com.intel.analytics.bigdl.tensor.DenseTensorDimApply$.dimApply2(DenseTensorDimApply.scala:47)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$.sum(DenseTensorMath.scala:468)
	at com.intel.analytics.bigdl.tensor.DenseTensor.sum(DenseTensor.scala:819)
	at com.intel.analytics.bigdl.nn.CMul.accGradParameters(CMul.scala:153)
	at com.intel.analytics.bigdl.nn.CMul.accGradParameters(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:285)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:62)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateGradInput(NetUtils.scala:70)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-28 02:52:53 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.lang.Float.valueOf(Float.java:433)
	at scala.runtime.BoxesRunTime.boxToFloat(BoxesRunTime.java:73)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.sum(TensorNumeric.scala:503)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$$anonfun$sum$2.apply(DenseTensorMath.scala:470)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$$anonfun$sum$2.apply(DenseTensorMath.scala:469)
	at com.intel.analytics.bigdl.tensor.DenseTensorDimApply$.dimApply2(DenseTensorDimApply.scala:47)
	at com.intel.analytics.bigdl.tensor.DenseTensorMath$.sum(DenseTensorMath.scala:468)
	at com.intel.analytics.bigdl.tensor.DenseTensor.sum(DenseTensor.scala:819)
	at com.intel.analytics.bigdl.nn.CMul.accGradParameters(CMul.scala:153)
	at com.intel.analytics.bigdl.nn.CMul.accGradParameters(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:285)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:62)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateGradInput(NetUtils.scala:70)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-28 02:55:07 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-28 02:55:14 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-28 02:55:14 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-28 02:55:18 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-28 02:55:18 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-28 02:55:18 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-28 02:55:18 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-28 02:55:18 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-28 02:55:18 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-28 02:55:18 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-28 02:55:18 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-28 02:55:18 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-28 02:55:18 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-28 02:55:18 INFO  DistriOptimizer$:148 - Count dataset
2019-10-28 02:55:18 INFO  DistriOptimizer$:148 - Count dataset
2019-10-28 02:55:18 INFO  DistriOptimizer$:152 - Count dataset complete. Time elapsed: 0.045537745s
2019-10-28 02:55:18 INFO  DistriOptimizer$:152 - Count dataset complete. Time elapsed: 0.045537745s
2019-10-28 02:55:18 INFO  DistriOptimizer$:160 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-10-28 02:55:18 INFO  DistriOptimizer$:160 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-10-28 02:55:18 INFO  DistriOptimizer$:164 - Shuffle data
2019-10-28 02:55:18 INFO  DistriOptimizer$:164 - Shuffle data
2019-10-28 02:55:18 INFO  DistriOptimizer$:167 - Shuffle data complete. Takes 0.07214632s
2019-10-28 02:55:18 INFO  DistriOptimizer$:167 - Shuffle data complete. Takes 0.07214632s
2019-10-28 02:55:22 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resizeAs(DenseTensor.scala:2464)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resizeAs(DenseTensor.scala:95)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:116)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:62)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateGradInput(NetUtils.scala:70)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-28 02:55:22 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resizeAs(DenseTensor.scala:2464)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resizeAs(DenseTensor.scala:95)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:116)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:62)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateGradInput(NetUtils.scala:70)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-28 02:55:22 ERROR SparkUncaughtExceptionHandler:91 - Uncaught exception in thread Thread[Executor task launch worker for task 5,5,main]
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resizeAs(DenseTensor.scala:2464)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resizeAs(DenseTensor.scala:95)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:116)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:62)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateGradInput(NetUtils.scala:70)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-28 02:55:22 ERROR SparkUncaughtExceptionHandler:91 - Uncaught exception in thread Thread[Executor task launch worker for task 5,5,main]
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resizeAs(DenseTensor.scala:2464)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resizeAs(DenseTensor.scala:95)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:116)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:62)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateGradInput(NetUtils.scala:70)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-28 02:55:23 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resizeAs(DenseTensor.scala:2464)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resizeAs(DenseTensor.scala:95)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:116)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:62)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateGradInput(NetUtils.scala:70)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-28 02:55:23 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resizeAs(DenseTensor.scala:2464)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resizeAs(DenseTensor.scala:95)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:116)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:62)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateGradInput(NetUtils.scala:70)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-28 02:55:24 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-10-28 02:55:24 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-10-28 02:55:27 WARN  SparkContext:87 - Ignoring Exception while stopping SparkContext from shutdown hook
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 02:55:27 WARN  SparkContext:87 - Ignoring Exception while stopping SparkContext from shutdown hook
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 02:55:55 WARN  SparkContext:87 - Ignoring Exception while stopping SparkContext from shutdown hook
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 02:55:55 WARN  SparkContext:87 - Ignoring Exception while stopping SparkContext from shutdown hook
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 02:55:55 ERROR Utils:91 - Uncaught exception in thread Thread-1
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 02:55:55 ERROR Utils:91 - Uncaught exception in thread Thread-1
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 02:55:55 WARN  ShutdownHookManager:56 - ShutdownHook '$anon$2' failed, java.lang.OutOfMemoryError: GC overhead limit exceeded
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 02:55:55 WARN  ShutdownHookManager:56 - ShutdownHook '$anon$2' failed, java.lang.OutOfMemoryError: GC overhead limit exceeded
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 02:57:28 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-28 02:57:37 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-28 02:57:37 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-28 02:57:42 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-28 02:57:42 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-28 02:57:42 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-28 02:57:42 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-28 02:57:42 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-28 02:57:42 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-28 02:57:42 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-28 02:57:42 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-28 02:57:42 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-28 02:57:42 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-28 02:57:42 INFO  DistriOptimizer$:148 - Count dataset
2019-10-28 02:57:42 INFO  DistriOptimizer$:148 - Count dataset
2019-10-28 02:57:42 INFO  DistriOptimizer$:152 - Count dataset complete. Time elapsed: 0.066255255s
2019-10-28 02:57:42 INFO  DistriOptimizer$:152 - Count dataset complete. Time elapsed: 0.066255255s
2019-10-28 02:57:42 INFO  DistriOptimizer$:160 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-10-28 02:57:42 INFO  DistriOptimizer$:160 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-10-28 02:57:42 INFO  DistriOptimizer$:164 - Shuffle data
2019-10-28 02:57:42 INFO  DistriOptimizer$:164 - Shuffle data
2019-10-28 02:57:42 INFO  DistriOptimizer$:167 - Shuffle data complete. Takes 0.046101535s
2019-10-28 02:57:42 INFO  DistriOptimizer$:167 - Shuffle data complete. Takes 0.046101535s
2019-10-28 02:57:42 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy$mcF$sp(TensorNumeric.scala:721)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:715)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:503)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copy(MiniBatch.scala:460)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copyWithPadding(MiniBatch.scala:380)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:209)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:215)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:205)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-28 02:57:42 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy$mcF$sp(TensorNumeric.scala:721)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:715)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:503)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copy(MiniBatch.scala:460)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copyWithPadding(MiniBatch.scala:380)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:209)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:215)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:205)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-28 02:57:42 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy$mcF$sp(TensorNumeric.scala:721)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:715)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:503)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copy(MiniBatch.scala:460)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copyWithPadding(MiniBatch.scala:380)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:209)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:215)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:205)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-28 02:57:42 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy$mcF$sp(TensorNumeric.scala:721)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:715)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:503)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copy(MiniBatch.scala:460)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copyWithPadding(MiniBatch.scala:380)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:209)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:215)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:205)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-28 02:57:42 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-10-28 02:57:42 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-10-28 02:57:42 ERROR DistriOptimizer$:907 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy$mcF$sp(TensorNumeric.scala:721)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:715)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:503)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copy(MiniBatch.scala:460)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copyWithPadding(MiniBatch.scala:380)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:209)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:215)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:205)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:333)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:427)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy$mcF$sp(TensorNumeric.scala:721)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:715)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:503)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copy(MiniBatch.scala:460)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copyWithPadding(MiniBatch.scala:380)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:209)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:215)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:205)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-10-28 02:57:42 ERROR DistriOptimizer$:907 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy$mcF$sp(TensorNumeric.scala:721)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:715)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:503)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copy(MiniBatch.scala:460)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copyWithPadding(MiniBatch.scala:380)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:209)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:215)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:205)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:333)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:427)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy$mcF$sp(TensorNumeric.scala:721)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:715)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:503)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copy(MiniBatch.scala:460)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copyWithPadding(MiniBatch.scala:380)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:209)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:215)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:205)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-10-28 02:59:36 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-28 02:59:42 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-28 02:59:42 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-28 02:59:45 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-28 02:59:45 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-28 02:59:46 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-28 02:59:46 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-28 02:59:46 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-28 02:59:46 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-28 02:59:46 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-28 02:59:46 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-28 02:59:46 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-28 02:59:46 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-28 02:59:46 INFO  DistriOptimizer$:148 - Count dataset
2019-10-28 02:59:46 INFO  DistriOptimizer$:148 - Count dataset
2019-10-28 02:59:46 INFO  DistriOptimizer$:152 - Count dataset complete. Time elapsed: 0.045124041s
2019-10-28 02:59:46 INFO  DistriOptimizer$:152 - Count dataset complete. Time elapsed: 0.045124041s
2019-10-28 02:59:46 INFO  DistriOptimizer$:160 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-10-28 02:59:46 INFO  DistriOptimizer$:160 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-10-28 02:59:46 INFO  DistriOptimizer$:164 - Shuffle data
2019-10-28 02:59:46 INFO  DistriOptimizer$:164 - Shuffle data
2019-10-28 02:59:46 INFO  DistriOptimizer$:167 - Shuffle data complete. Takes 0.021387419s
2019-10-28 02:59:46 INFO  DistriOptimizer$:167 - Shuffle data complete. Takes 0.021387419s
2019-10-28 02:59:50 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resizeAs(DenseTensor.scala:2464)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resizeAs(DenseTensor.scala:95)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:116)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:62)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateGradInput(NetUtils.scala:70)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-28 02:59:50 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resizeAs(DenseTensor.scala:2464)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resizeAs(DenseTensor.scala:95)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:116)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:62)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateGradInput(NetUtils.scala:70)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-28 03:11:46 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-28 03:15:56 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-28 03:21:03 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-28 03:21:11 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-28 03:21:11 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-28 03:21:15 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-28 03:21:15 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-28 03:21:15 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-28 03:21:15 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-28 03:21:15 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-28 03:21:15 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-28 03:21:15 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-28 03:21:15 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-28 03:21:15 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-28 03:21:15 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-28 03:21:15 INFO  DistriOptimizer$:148 - Count dataset
2019-10-28 03:21:15 INFO  DistriOptimizer$:148 - Count dataset
2019-10-28 03:21:15 INFO  DistriOptimizer$:152 - Count dataset complete. Time elapsed: 0.056472943s
2019-10-28 03:21:15 INFO  DistriOptimizer$:152 - Count dataset complete. Time elapsed: 0.056472943s
2019-10-28 03:21:15 INFO  DistriOptimizer$:160 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-10-28 03:21:15 INFO  DistriOptimizer$:160 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-10-28 03:21:15 INFO  DistriOptimizer$:164 - Shuffle data
2019-10-28 03:21:15 INFO  DistriOptimizer$:164 - Shuffle data
2019-10-28 03:21:15 INFO  DistriOptimizer$:167 - Shuffle data complete. Takes 0.030449801s
2019-10-28 03:21:15 INFO  DistriOptimizer$:167 - Shuffle data complete. Takes 0.030449801s
2019-10-28 03:21:19 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resizeAs(DenseTensor.scala:2464)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resizeAs(DenseTensor.scala:95)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:116)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:62)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateGradInput(NetUtils.scala:70)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-28 03:21:19 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resizeAs(DenseTensor.scala:2464)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resizeAs(DenseTensor.scala:95)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:116)
	at com.intel.analytics.bigdl.nn.CMul.updateGradInput(CMul.scala:41)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:62)
	at com.intel.analytics.bigdl.nn.Scale.updateGradInput(Scale.scala:37)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateGradInput(NetUtils.scala:70)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.StaticGraph.backwardExecution(StaticGraph.scala:142)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateGradInput(StaticGraph.scala:78)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateGradInput(KerasLayer.scala:192)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.backward(AbstractModule.scala:284)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-28 03:21:47 ERROR Utils:91 - uncaught error in thread Spark Context Cleaner, stopping SparkContext
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 03:21:47 ERROR Utils:91 - uncaught error in thread Spark Context Cleaner, stopping SparkContext
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 03:21:48 ERROR Utils:91 - Uncaught exception in thread driver-heartbeater
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 03:21:48 ERROR Utils:91 - Uncaught exception in thread driver-heartbeater
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 03:21:48 ERROR Utils:91 - uncaught error in thread spark-listener-group-appStatus, stopping SparkContext
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 03:21:48 ERROR DistriOptimizer$:907 - Error: org.apache.spark.SparkException: Job 4 cancelled as part of cancellation of all jobs
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1824)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:830)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:830)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:830)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:830)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2082)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:333)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:427)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

2019-10-28 03:21:48 ERROR DistriOptimizer$:907 - Error: org.apache.spark.SparkException: Job 4 cancelled as part of cancellation of all jobs
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1824)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:830)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:830)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:830)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:830)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2082)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:333)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:427)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

2019-10-28 03:21:48 ERROR Utils:91 - throw uncaught fatal error in thread Spark Context Cleaner
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 03:21:48 ERROR Utils:91 - throw uncaught fatal error in thread Spark Context Cleaner
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 03:21:52 WARN  SparkContext:87 - Ignoring Exception while stopping SparkContext from shutdown hook
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 03:21:52 WARN  SparkContext:87 - Ignoring Exception while stopping SparkContext from shutdown hook
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 03:21:53 WARN  NioEventLoop:151 - Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 03:21:53 WARN  NioEventLoop:151 - Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 03:21:54 ERROR Utils:91 - Uncaught exception in thread Thread-1
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 03:21:54 ERROR Utils:91 - Uncaught exception in thread Thread-1
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 03:21:55 WARN  ShutdownHookManager:56 - ShutdownHook '$anon$2' failed, java.lang.OutOfMemoryError: GC overhead limit exceeded
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 03:21:55 WARN  ShutdownHookManager:56 - ShutdownHook '$anon$2' failed, java.lang.OutOfMemoryError: GC overhead limit exceeded
java.lang.OutOfMemoryError: GC overhead limit exceeded
2019-10-28 03:23:52 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-28 03:23:59 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-28 03:23:59 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-28 03:24:03 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-28 03:24:03 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-28 03:24:03 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-28 03:24:03 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-28 03:24:03 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-28 03:24:03 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-28 03:24:03 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-28 03:24:03 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-28 03:24:03 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-28 03:24:03 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-28 03:24:03 INFO  DistriOptimizer$:148 - Count dataset
2019-10-28 03:24:03 INFO  DistriOptimizer$:148 - Count dataset
2019-10-28 03:24:03 INFO  DistriOptimizer$:152 - Count dataset complete. Time elapsed: 0.147144046s
2019-10-28 03:24:03 INFO  DistriOptimizer$:152 - Count dataset complete. Time elapsed: 0.147144046s
2019-10-28 03:24:03 INFO  DistriOptimizer$:160 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-10-28 03:24:03 INFO  DistriOptimizer$:160 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-10-28 03:24:03 INFO  DistriOptimizer$:164 - Shuffle data
2019-10-28 03:24:03 INFO  DistriOptimizer$:164 - Shuffle data
2019-10-28 03:24:03 INFO  DistriOptimizer$:167 - Shuffle data complete. Takes 0.024693286s
2019-10-28 03:24:03 INFO  DistriOptimizer$:167 - Shuffle data complete. Takes 0.024693286s
2019-10-28 03:24:05 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
Layer info: Model[ae513ecc]/KerasLayerWrapper[GraphNete6ced1b9_wrapper]/SpatialConvolution[conv4_2/dwise](192 -> 192, 3 x 3, 1, 1, 1, 1)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resize(DenseTensor.scala:2473)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resize(DenseTensor.scala:125)
	at com.intel.analytics.bigdl.nn.SpatialConvolution.updateOutput(SpatialConvolution.scala:324)
	at com.intel.analytics.bigdl.nn.SpatialConvolution.updateOutput(SpatialConvolution.scala:54)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-28 03:24:05 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
Layer info: Model[ae513ecc]/KerasLayerWrapper[GraphNete6ced1b9_wrapper]/SpatialConvolution[conv4_2/dwise](192 -> 192, 3 x 3, 1, 1, 1, 1)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resize(DenseTensor.scala:2473)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resize(DenseTensor.scala:125)
	at com.intel.analytics.bigdl.nn.SpatialConvolution.updateOutput(SpatialConvolution.scala:324)
	at com.intel.analytics.bigdl.nn.SpatialConvolution.updateOutput(SpatialConvolution.scala:54)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-28 03:24:05 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): Layer info: Model[ae513ecc]/KerasLayerWrapper[GraphNete6ced1b9_wrapper]/SpatialConvolution[conv4_2/dwise](192 -> 192, 3 x 3, 1, 1, 1, 1)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resize(DenseTensor.scala:2473)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resize(DenseTensor.scala:125)
	at com.intel.analytics.bigdl.nn.SpatialConvolution.updateOutput(SpatialConvolution.scala:324)
	at com.intel.analytics.bigdl.nn.SpatialConvolution.updateOutput(SpatialConvolution.scala:54)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-28 03:24:05 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): Layer info: Model[ae513ecc]/KerasLayerWrapper[GraphNete6ced1b9_wrapper]/SpatialConvolution[conv4_2/dwise](192 -> 192, 3 x 3, 1, 1, 1, 1)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resize(DenseTensor.scala:2473)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resize(DenseTensor.scala:125)
	at com.intel.analytics.bigdl.nn.SpatialConvolution.updateOutput(SpatialConvolution.scala:324)
	at com.intel.analytics.bigdl.nn.SpatialConvolution.updateOutput(SpatialConvolution.scala:54)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-28 03:24:05 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-10-28 03:24:05 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-10-28 03:24:05 ERROR DistriOptimizer$:907 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): Layer info: Model[ae513ecc]/KerasLayerWrapper[GraphNete6ced1b9_wrapper]/SpatialConvolution[conv4_2/dwise](192 -> 192, 3 x 3, 1, 1, 1, 1)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resize(DenseTensor.scala:2473)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resize(DenseTensor.scala:125)
	at com.intel.analytics.bigdl.nn.SpatialConvolution.updateOutput(SpatialConvolution.scala:324)
	at com.intel.analytics.bigdl.nn.SpatialConvolution.updateOutput(SpatialConvolution.scala:54)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:333)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:427)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Model[ae513ecc]/KerasLayerWrapper[GraphNete6ced1b9_wrapper]/SpatialConvolution[conv4_2/dwise](192 -> 192, 3 x 3, 1, 1, 1, 1)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resize(DenseTensor.scala:2473)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resize(DenseTensor.scala:125)
	at com.intel.analytics.bigdl.nn.SpatialConvolution.updateOutput(SpatialConvolution.scala:324)
	at com.intel.analytics.bigdl.nn.SpatialConvolution.updateOutput(SpatialConvolution.scala:54)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-10-28 03:24:05 ERROR DistriOptimizer$:907 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): Layer info: Model[ae513ecc]/KerasLayerWrapper[GraphNete6ced1b9_wrapper]/SpatialConvolution[conv4_2/dwise](192 -> 192, 3 x 3, 1, 1, 1, 1)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resize(DenseTensor.scala:2473)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resize(DenseTensor.scala:125)
	at com.intel.analytics.bigdl.nn.SpatialConvolution.updateOutput(SpatialConvolution.scala:324)
	at com.intel.analytics.bigdl.nn.SpatialConvolution.updateOutput(SpatialConvolution.scala:54)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:333)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:427)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: Layer info: Model[ae513ecc]/KerasLayerWrapper[GraphNete6ced1b9_wrapper]/SpatialConvolution[conv4_2/dwise](192 -> 192, 3 x 3, 1, 1, 1, 1)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:133)
	at scala.reflect.ManifestFactory$$anon$11.newArray(Manifest.scala:131)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.rawResize(DenseTensor.scala:2409)
	at com.intel.analytics.bigdl.tensor.DenseTensor$.resize(DenseTensor.scala:2473)
	at com.intel.analytics.bigdl.tensor.DenseTensor.resize(DenseTensor.scala:125)
	at com.intel.analytics.bigdl.nn.SpatialConvolution.updateOutput(SpatialConvolution.scala:324)
	at com.intel.analytics.bigdl.nn.SpatialConvolution.updateOutput(SpatialConvolution.scala:54)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:265)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.zoo.pipeline.api.net.GraphNet.updateOutput(NetUtils.scala:65)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.nn.StaticGraph.updateOutput(StaticGraph.scala:62)
	at com.intel.analytics.bigdl.nn.keras.KerasLayer.updateOutput(KerasLayer.scala:187)
	at com.intel.analytics.bigdl.nn.abstractnn.AbstractModule.forward(AbstractModule.scala:259)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply$mcI$sp(DistriOptimizer.scala:256)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4$$anonfun$5$$anonfun$apply$2.apply(DistriOptimizer.scala:247)
	at com.intel.analytics.bigdl.utils.ThreadPool$$anonfun$1$$anon$5.call(ThreadPool.scala:156)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-10-28 03:26:38 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2019-10-28 03:26:48 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-28 03:26:48 INFO  DistriOptimizer$:791 - caching training rdd ...
2019-10-28 03:26:53 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-28 03:26:53 INFO  DistriOptimizer$:629 - Cache thread models...
2019-10-28 03:26:53 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-28 03:26:53 INFO  DistriOptimizer$:612 - model thread pool size is 1
2019-10-28 03:26:53 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-28 03:26:53 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2019-10-28 03:26:53 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-28 03:26:53 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
2019-10-28 03:26:53 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-28 03:26:53 INFO  DistriOptimizer$:631 - Cache thread models... done
2019-10-28 03:26:53 INFO  DistriOptimizer$:148 - Count dataset
2019-10-28 03:26:53 INFO  DistriOptimizer$:148 - Count dataset
2019-10-28 03:26:53 INFO  DistriOptimizer$:152 - Count dataset complete. Time elapsed: 0.067371835s
2019-10-28 03:26:53 INFO  DistriOptimizer$:152 - Count dataset complete. Time elapsed: 0.067371835s
2019-10-28 03:26:53 INFO  DistriOptimizer$:160 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-10-28 03:26:53 INFO  DistriOptimizer$:160 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-10-28 03:26:53 INFO  DistriOptimizer$:164 - Shuffle data
2019-10-28 03:26:53 INFO  DistriOptimizer$:164 - Shuffle data
2019-10-28 03:26:53 INFO  DistriOptimizer$:167 - Shuffle data complete. Takes 0.024279296s
2019-10-28 03:26:53 INFO  DistriOptimizer$:167 - Shuffle data complete. Takes 0.024279296s
2019-10-28 03:26:53 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy$mcF$sp(TensorNumeric.scala:721)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:715)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:503)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copy(MiniBatch.scala:460)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copyWithPadding(MiniBatch.scala:380)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:209)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:215)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:205)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-28 03:26:53 ERROR Executor:91 - Exception in task 0.0 in stage 9.0 (TID 5)
java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy$mcF$sp(TensorNumeric.scala:721)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:715)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:503)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copy(MiniBatch.scala:460)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copyWithPadding(MiniBatch.scala:380)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:209)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:215)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:205)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-28 03:26:54 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy$mcF$sp(TensorNumeric.scala:721)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:715)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:503)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copy(MiniBatch.scala:460)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copyWithPadding(MiniBatch.scala:380)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:209)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:215)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:205)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-28 03:26:54 WARN  TaskSetManager:66 - Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy$mcF$sp(TensorNumeric.scala:721)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:715)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:503)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copy(MiniBatch.scala:460)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copyWithPadding(MiniBatch.scala:380)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:209)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:215)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:205)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-28 03:26:54 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-10-28 03:26:54 ERROR TaskSetManager:70 - Task 0 in stage 9.0 failed 1 times; aborting job
2019-10-28 03:26:54 ERROR DistriOptimizer$:907 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy$mcF$sp(TensorNumeric.scala:721)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:715)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:503)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copy(MiniBatch.scala:460)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copyWithPadding(MiniBatch.scala:380)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:209)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:215)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:205)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:333)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:427)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy$mcF$sp(TensorNumeric.scala:721)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:715)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:503)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copy(MiniBatch.scala:460)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copyWithPadding(MiniBatch.scala:380)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:209)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:215)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:205)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-10-28 03:26:54 ERROR DistriOptimizer$:907 - Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 5, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy$mcF$sp(TensorNumeric.scala:721)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:715)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:503)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copy(MiniBatch.scala:460)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copyWithPadding(MiniBatch.scala:380)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:209)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:215)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:205)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1035)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1017)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$.optimize(DistriOptimizer.scala:333)
	at com.intel.analytics.bigdl.optim.DistriOptimizer.optimize(DistriOptimizer.scala:881)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:369)
	at com.intel.analytics.zoo.pipeline.api.keras.models.KerasNet.fit(Topology.scala:427)
	at com.intel.analytics.zoo.pipeline.api.keras.python.PythonZooKeras.zooFit(PythonZooKeras.scala:104)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy$mcF$sp(TensorNumeric.scala:721)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:715)
	at com.intel.analytics.bigdl.tensor.TensorNumericMath$TensorNumeric$NumericFloat$.arraycopy(TensorNumeric.scala:503)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copy(MiniBatch.scala:460)
	at com.intel.analytics.bigdl.dataset.MiniBatch$.copyWithPadding(MiniBatch.scala:380)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:209)
	at com.intel.analytics.bigdl.dataset.ArrayTensorMiniBatch.set(MiniBatch.scala:111)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:348)
	at com.intel.analytics.bigdl.dataset.SampleToMiniBatch$$anon$2.next(Transformer.scala:323)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:215)
	at com.intel.analytics.bigdl.optim.DistriOptimizer$$anonfun$4.apply(DistriOptimizer.scala:205)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more

2019-10-28 03:28:47 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
